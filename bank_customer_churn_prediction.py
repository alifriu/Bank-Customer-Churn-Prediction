# -*- coding: utf-8 -*-
"""Bank Customer Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f4BZjPoqI5ouRnv8Ciiw9WUWhBrLEy2X

# Proyek Machine Learning System Design: Bank Customer Churn Prediction
- **Nama:** Muhammad Alif Alfattah Riu
- **Email:** alifalfattah12@gmail.com
- **ID Dicoding:** alif_riu

# **Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
# Memanggil beberapa library dan packages yang digunakan di dalam proyek
import kagglehub
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.utils import resample
from sklearn.preprocessing import  OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""# **Data Loading**"""

# Download latest version
path = kagglehub.dataset_download("shubhammeshram579/bank-customer-churn-prediction")

print("Path to dataset files:", path)

# membaca dataset
ds = '/root/.cache/kagglehub/datasets/shubhammeshram579/bank-customer-churn-prediction/versions/1/Churn_Modelling.csv'
df = pd.read_csv(ds)
df.head()

"""# **Exploratory Data Analysis**

__Exploratory Data Analysis (EDA)__  adalah proses untuk memahami karakteristik data sebelum melakukan pemodelan.

## **EDA - Dekripsi Variabel**
"""

# Melihat jumlah data pengamatan yang ada,jumlah kolom,tipe data pada masing masing kolom, dan memeriksa apakah ada missing value
df.info()

# memeriksa nilai dari statistik deskriptif pada dataset seperti mean,median,standar deviasi,dll.
df.describe()

# memeriksa jumlah missing value pada masing masing variabel
df.isnull().sum()

# memeriksa apakah ada data duplikat
df.duplicated().sum()

# melihat jumlah baris dan kolom pada dataset
df.shape

# variabel RowNumber,CustomerId, dan Surname akan di hapus karena tidak akan digunakan pada tahap analisis
df.drop(['RowNumber', 'CustomerId', 'Surname'], inplace=True, axis=1)
df

"""## **EDA - Handling Missing Value and Duplicate Value**

Mengisi nilai missing value dengan menggunakan metode ffill.Ini bertujuan untuk mengisi nilai baris yang kosong pada kolom `HasCrCard` dan `IsActiveMember` dengan nilai baris sebelumnya.
"""

df['HasCrCard'] = df['HasCrCard'].fillna(method='ffill')
df['IsActiveMember'] = df['IsActiveMember'].fillna(method='ffill')

"""Variabel `Geography` bertipe data kategorikal sehingga dapat diisi nilainya dengan menggunakan modus yang ada pada kolom.Ini bertujuan untuk memperkecil Variasi data yang ada pada kolom."""

geography_mode = df['Geography'].mode()[0]
df['Geography'].fillna(geography_mode, inplace=True)

"""Dikarenakan variabel `Age` bertipe numerik, dapat dilakukan proses handling missing value dengan menggantinya denga nilai median.Metode ini bertujuan untuk meminimalisir magnitude dari skew dan outlier pada kolom."""

age_median = df['Age'].median()
df['Age'].fillna(age_median, inplace=True)

# pengecekan kembali nilai missing value pada tiap kolom
df.isnull().sum()

# drop baris yang memiliki nilai duplikat dari baris lain dan mengecek apakah nilai duplikat masih ada atau tidak
df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""## **EDA - Univariate Analysis**

Analisis ini memeriksa satu variabel pada satu waktu. Tujuannya untuk memahami distribusi, nilai rata-rata, varians, serta mendeteksi outliers.

### Categorical Features
"""

# memisahkan kolom bertipe kategorikal
categorical_features = ['Geography', 'Gender']

# Visualisasi persebaran data pada kolom kategorikal
plt.figure(figsize=(12, 6))
for i, feature in enumerate(categorical_features):
    ax = plt.subplot(1, len(categorical_features), i + 1)
    sns.countplot(data=df, x=feature, ax=ax, palette="Set2",hue=feature)
    plt.xticks(rotation=45, ha='right')
plt.suptitle("Distribusi dari Categorical Features", fontsize=14)
plt.tight_layout()
plt.show()

"""__Insight__ : Distribusi dari dataset yang ada menunjukan bahwa `Geography` asal dari pelanggan berasal dari 3 negara yaitu France,Spain, dan Germany.
    Pada variabel `Gender`,terlihat bahwa jumlah pelanggan pria lebih dominan dibandingkan dengan pelanggan wanita.

### Numerical Features
"""

# memisahkan data bertipe numerik
numerical_features = ['CreditScore','Age','Tenure','Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']

# visualisasi pada variabel Exited/churn
sns.countplot(x='Exited', data=df, palette='Set2',hue='Exited')
plt.title('Distribusi Exited/Churn ')

"""__Insight:__ Distribusi dari pelanggan yang sudah tidak berlangganan (1) terhadap pelanggan yang masih berlangganan (0) terlihat bahwa masih banyak pelanggan yang masih berlangganan."""

# visualisasi distribusi data pada tiap fitur numerik
def histplot_analysis(data, columns):
    plt.figure(figsize=(15, 12))

    for i, column in enumerate(columns, 1):
        plt.subplot(3, 3, i)  # Create a 3x3 grid of subplots
        sns.histplot(data[column], kde=True, bins=30, color='lightseagreen')
        plt.title(f'Distribusi {column.replace("_", " ")}')
        plt.xlabel(column.replace('_', ' '))
        plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

histplot_analysis(df, numerical_features)

""" Boxplot merupakan visualisasi yang efektif untuk mendeteksi data pencilan (outlier) pada variabel numerik. Dengan menampilkan kuartil data, boxplot memberikan gambaran jelas tentang sebaran data dan mengidentifikasi nilai-nilai ekstrem yang berada di luar jangkauan interkuartil. Titik-titik data yang terletak di luar 'kumis' boxplot umumnya dianggap sebagai outlier.
   Variabel Creditscore dan Age memiliki data outlier yang cukup banyak jika dibandingkan dengan variabel lainnya.
"""

# visualisasi menggunakan boxplot untuk mencari niai outlier
def boxplot_analysis(data, columns):
    plt.figure(figsize=(15, 12))

    for i, column in enumerate(columns, 1):
        plt.subplot(3, 3, i)
        sns.boxplot(x=data[column], color='lightseagreen')
        plt.title(f'{column.replace("_", " ")} Boxplot')
        plt.xlabel(column.replace('_', ' '))


    plt.tight_layout()
    plt.show()

boxplot_analysis(df, numerical_features)

"""## **EDA - Bivariate Analysis**

Analisis ini mengevaluasi hubungan antara dua variabel. Tujuan utamanya adalah untuk menemukan korelasi atau pola antara dua fitur.

### **Categorical Features**
"""

# visualisasi distribusi Exited/churn pelanggan berdasarkan geography
sns.countplot(data=df,x='Geography',hue='Exited',palette='Set2')
plt.xticks(rotation=45, ha='right')
plt.title('Distribusi Exited/Churn berdasarkan Geography')

"""__Insight:__ bedasarkan persebaran churn berdasarkan geography,France menjadi negara terbanyak memiliki pelanggan loyal dan churn disaat yang bersamaan."""

# visualisasi distribusi exited/churn pelanggan berdasarkan gender
sns.countplot(data=df,x='Gender',hue='Exited',palette='Set2')
plt.xticks(rotation=45, ha='right')
plt.title('Distribusi Exited/Churn berdasarkan Gender')

"""__Insight:__ bedasarkan persebaran churn berdasarkan Gender,Wanita sedikit lebih banyak yang berhenti menggunakan layanan dibandingkan pria.

### **Numerical Features**
"""

# visualisasi kepadatan pelanggan exited/churn berdasarkan fitur numerik
def density_plot(columns):
    plt.figure(figsize=(18, 15))
    num_plots = len(numerical_features)
    rows, cols = 3, min(3, num_plots)

    for i, column in enumerate(numerical_features):
        ax = plt.subplot(rows, cols, i + 1)
        sns.histplot(data=df, x=column, hue='Exited', kde=True, fill=True, palette='Set2', ax=ax)
        ax.set_title(f'Kepadatan Exited/Chrun berdasarkan {column}', fontsize=16)
        ax.set_xlabel(f'{column}')

    plt.tight_layout()
    plt.show()

density_plot(numerical_features)

"""Density plot merupakan visualisasi yang berguna untuk menggambarkan distribusi dari kepadatan suatu variabel.Berdasarkan data yang ada,kepadatan dari pelanggan yang tidak churn pada masing masing variabel lebih banyak jika dibandingkan dengan pelanggan yang churn.

## **EDA - Multivariate Analysis**

Analisis ini melibatkan lebih dari dua variabel sekaligus. Ini bertujuan untuk melihat interaksi kompleks antar variabel dan pola tersembunyi dalam data.

### **Categorical Features**
"""

# visualisasi DIstibusi Pelanggan yang mengalami churn di tiap negara berdasarkan rata rata saldo
sns.axes_style('whitegrid')
g = sns.catplot(x='Geography', y='Balance', data=df, hue='Exited', kind='bar', palette='Set2', alpha=0.8, height=5)
plt.title('Distibusi Pelanggan yang mengalami churn di tiap negara berdasarkan rata rata saldo')
g.set_axis_labels('Country', 'Balance')
plt.xticks(rotation=45, ha='right')
plt.show()

"""__Insight:__ Dari Rata-rata saldo pelangan di tiap Negara,German menjadi negarara yang memiliki rata-rata saldo tertinggi diantara dua negara yang lain.

### **Numerical Features**
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

numerical_features = ['CreditScore','Age','Tenure','Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']
categorical_features = ['Geography', 'Gender']

# membuat heatmap untuk melihat korelasi antar fitur numerik
plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='Set2', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""__Insight:__ berdasarkan visualisasi diatas,hubungan korelasi antar variabel bisa dibilang cukup rendah.

# **Data Preparation**

## **Enkoding Fitur Kategorical**

Untuk memungkinkan algoritma machine learning memproses kolom bertipe data kategorikal seperti 'Geography' dan 'Gender', kita perlu mengubahnya menjadi format numerik. Proses ini disebut encoding. Setiap kategori unik dalam variabel tersebut akan diubah menjadi vektor biner, di mana hanya satu nilai yang akan bernilai 1 (true) dan sisanya 0. Dengan demikian, informasi dari variabel kategorikal dapat dimasukkan secara efektif ke dalam model.
"""

df = pd.get_dummies(df, columns=['Geography','Gender'],dtype=int)
df

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='Set2', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""## **Oversample minority data class**

Teknik resample data adalah suatu metode yang digunakan dalam pra-pemrosesan data, khususnya dalam Machine Learning, untuk mengatasi ketidakseimbangan kelas (imbalanced class) dalam dataset. Ketidakseimbangan kelas terjadi ketika jumlah data pada satu kelas jauh lebih banyak daripada kelas lainnya. Kondisi ini dapat menyebabkan model Machine Learning menjadi bias terhadap kelas mayoritas dan mengabaikan kelas minoritas.
"""

df['Exited'].value_counts()

"""Pada data yang tersedia, label data pelanggan yang tidak churn (0) lebih dominan dibandingkan dengan label data pelanggan yang churn sehingga kita perlu melakukan proses Resample pada label data pelanggan `churn = 1`"""

majority_class = df[df['Exited'] == 0]
minority_class = df[df['Exited'] == 1]

# Oversample the minority class
minority_oversampled = resample(minority_class,
                                replace=True,
                                n_samples=len(majority_class),
                                random_state=42)

# Combine the majority class with the oversampled minority class
df = pd.concat([majority_class, minority_oversampled])

df['Exited'].value_counts()

"""setelah dilakukan teknik resample, kedua label menjadi balance sehingga siap untuk masuk ke pemrosesan data tahap berikutnya

## **Splitting dan Standardisasi data**

Tujuan utama dari kasus ini  adalah memprediksi apakah pelanggan akan churn atau tidak (variabel 'Exited'). Untuk mencapai tujuan ini, kita akan membagi data menjadi dua bagian: 80% untuk melatih model (train set) dan 20% untuk menguji performanya (test set). Train set digunakan untuk mengajarkan model mengenali pola yang mengindikasikan churn, sedangkan test set digunakan untuk mengukur seberapa akurat model dalam memprediksi pelanggan baru yang akan churn.
"""

X = df.drop('Exited', axis=1)
y = df['Exited']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)

print(f"Training Set: {X_train.shape}, {y_train.shape}")
print(f"Testing Set: {X_test.shape}, {y_test.shape}")

"""Standarisasi data menggunakan StandardScaler bertujuan untuk menormalkan distribusi setiap fitur agar memiliki rata-rata sama dengan nol dan penyimpangan baku sama dengan satu. Teknik ini memastikan bahwa semua fitur berkontribusi secara setara dalam model."""

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **Modelling**

Model yang digunakan dalam proses pemodelan antara lain Decision Tree Classifier,Support Vector Machine, dan Random Forest Classifier.

Metrik Evaluasi yang akan diukur adalah Accuracy pada data train dan data test
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['Train_Accuracy', 'test_Accuracy'],
                      columns=['DecisionTree', 'SVM', 'RandomForest',])

"""## **Decision Tree**"""

dec_tree = DecisionTreeClassifier()

kombinasi_hyperparams_dectree = {
    'max_depth' : [None,2,3,4],
    'min_samples_leaf':[10,20,50],
}
dec_tree = GridSearchCV(dec_tree,
                          param_grid=kombinasi_hyperparams_dectree,
                          n_jobs=-1,
                          verbose=1)

dec_tree.fit(X_train, y_train)

y_test_pred = dec_tree.predict(X_test)
y_train_pred = dec_tree.predict(X_train)

models.loc['train_Accuracy','DecisionTree'] = accuracy_score(y_pred = y_train_pred, y_true=y_train)

print('----Accuracy Score----')
print('Hyper perameter terbaik : ',dec_tree.best_params_)
print('Accuracy Training : {:.2f}'.format(accuracy_score(y_train, y_train_pred) * 100), '%')
print('Accuracy Testing : {:.2f}'.format(accuracy_score(y_test, y_test_pred) * 100), '%')

print(classification_report(y_test, y_test_pred))

c_matrix_test_1 = confusion_matrix(y_test, y_test_pred)
print('Confusion Matrix Testing : \n', c_matrix_test_1)
print('True Positives :', c_matrix_test_1[0][0])
print('False Negatives :', c_matrix_test_1[0][1])
print('False Positives :', c_matrix_test_1[1][0])
print('True Negatives :', c_matrix_test_1[1][1])

matrix = pd.DataFrame(c_matrix_test_1, index=['Actual Positive: 0', 'Actual Negative: 1'], columns=['Predicted Positive: 0', 'Predicted Negative: 1'])
sns.heatmap(matrix, annot=True, fmt='d', cmap='Set2')

"""## **Support Vector Machine**"""

SVM = SVC()

kombinasi_hyperparams_svm = {
    'C' : [0.001, 0.01, 0.1, 1],
    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma' : ['scale','auto']
}

SVM = GridSearchCV(SVM,
                   param_grid=kombinasi_hyperparams_svm,
                   n_jobs=-1,
                   verbose=1)

SVM.fit(X_train, y_train)

y_test_pred = SVM.predict(X_test)
y_train_pred = SVM.predict(X_train)
models.loc['train_Accuracy','SVM'] = accuracy_score(y_pred = y_train_pred, y_true=y_train)

print('----Accuracy Score----')
print('Hyper perameter terbaik : ',SVM.best_params_)
print('Accuracy Training : {:.2f}'.format(accuracy_score(y_train, y_train_pred) * 100), '%')
print('Accuracy Testing : {:.2f}'.format(accuracy_score(y_test, y_test_pred) * 100), '%')

print(classification_report(y_test, y_test_pred))

c_matrix_test_2 = confusion_matrix(y_test, y_test_pred)
print('Confusion Matrix Testing : \n', c_matrix_test_2)
print('True Positives :', c_matrix_test_2[0][0])
print('False Negatives :', c_matrix_test_2[0][1])
print('False Positives :', c_matrix_test_2[1][0])
print('True Negatives :', c_matrix_test_2[1][1])

matrix = pd.DataFrame(c_matrix_test_2, index=['Actual Positive: 0', 'Actual Negative: 1'], columns=['Predicted Positive: 0', 'Predicted Negative: 1'])
sns.heatmap(matrix, annot=True, fmt='d', cmap='Set2')

"""## **Random Forest**"""

RF = RandomForestClassifier()

kombinasi_hyperparams_rf = {
    'n_estimators':[50,100,200],
    'max_depth' : [None,2,3,4],
    'min_samples_leaf':[10,20,50],
}

RF = GridSearchCV(RF,
                  param_grid=kombinasi_hyperparams_rf,
                  n_jobs=-1,
                  verbose=1)

RF.fit(X_train, y_train)

y_test_pred = RF.predict(X_test)
y_train_pred = RF.predict(X_train)

models.loc['train_Accuracy','RandomForest'] = accuracy_score(y_pred = y_train_pred, y_true=y_train)

print('----Accuracy Score----')
print('Hyper perameter terbaik : ',RF.best_params_)
print('Accuracy Training : {:.2f}'.format(accuracy_score(y_train, y_train_pred) * 100), '%')
print('Accuracy Testing : {:.2f}'.format(accuracy_score(y_test, y_test_pred) * 100), '%')

print(classification_report(y_test, y_test_pred))

c_matrix_test_3 = confusion_matrix(y_test, y_test_pred)
print('Confusion Matrix Testing : \n', c_matrix_test_3)
print('True Positives :', c_matrix_test_3[0][0])
print('False Negatives :', c_matrix_test_3[0][1])
print('False Positives :', c_matrix_test_3[1][0])
print('True Negatives :', c_matrix_test_3[1][1])

matrix = pd.DataFrame(c_matrix_test_3, index=['Actual Positive: 0', 'Actual Negative: 1'], columns=['Predicted Positive: 0', 'Predicted Negative: 1'])
sns.heatmap(matrix, annot=True, fmt='d', cmap='Set2')

"""# **Evaluasi Model**"""

acc = pd.DataFrame(columns=['train', 'test'], index=['DecisionTree','SVM','RandomForest'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'DecisionTree': dec_tree, 'SVM': SVM, 'RandomForest': RF}

for name, model in model_dict.items():
    acc.loc[name, 'train'] = accuracy_score(y_true=y_train, y_pred=model.predict(X_train))
    acc.loc[name, 'test'] = accuracy_score(y_true=y_test, y_pred=model.predict(X_test))

# Panggil acc
acc

# Melakukan pivot agar data cocok untuk Seaborn
acc_sorted = acc.sort_values(by='test', ascending=False)
acc_long = acc_sorted.reset_index().melt(id_vars='index', value_vars=['train', 'test'], var_name='Type', value_name='Score')

# Membuat plot dengan Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(
    data=acc_long,
    y='index',
    x='Score',
    hue='Type',
    palette='Set2',
    orient='h'
)

# Menambahkan grid, label, dan judul
plt.grid(axis='x', zorder=0)
plt.xlabel("Score")
plt.ylabel("Models")
plt.title("Train vs Test Scores Sorted by Test Score")
plt.legend(title='Score Type')

plt.show()

"""Secara keseluruhan, Random Forest menunjukkan performa terbaik dalam akurasi pada set test dan memiliki F1-Score tinggi untuk kedua kelas.Sementara itu, Support Vector Machine memiliki performa terendah dari segi akurasi.Decision Tree memberikan keseimbangan yang cukup baik antara precision, recall, dan F1-Score dengan akurasi yang cukup baik di set train dan set test.

Berdasarkan model yang dipilih,Random Forest merupakan model Random Forest yang terbaik dalam mengidentifikasi pelanggan yang berpotensi churn.
"""